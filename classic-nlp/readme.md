# Question Answering and Machine Comprehension

1. [Pranav Rajpurkar et al.: SQuAD: 100,000+ Questions for Machine Comprehension of Text. EMNLP 2015.](./QA&MC/1606.05250v1.pdf)
2. [Minjoon Soo et al.: Bi-Directional Attention Flow for Machine Comprehension. ICLR 2015.](./QA&MC/1611.01603v6.pdf)


# Automatic Text Summarization

1. [Kevin Knight and Daniel Marcu: Summarization beyond sentence extraction. Artificial Intelligence 139, 2002.](./ATS/1-s2.0-S0004370202002229-main.pdf)
2. [James Clarke and Mirella Lapata: Modeling Compression with Discourse Constraints. EMNLP-CONLL 2007.](./ATS/clarke07.pdf)
3. [Wen-tau Yih et al.: Multi-Document Summarization by Maximizing Informative Content-Words. IJCAI 2007.](./ATS/287.pdf)
4. [Alexander M Rush, et al.: A Neural Attention Model for Sentence Summarization.](./ATS/1509.00685.pdf)
5. [Abigail See et al.: Get To The Point: Summarization with Pointer-Generator Networks. ACL 2017.](./ATS/P17-1099.pdf)


# Coreference Resolution

1. [Vincent Ng: Supervised Noun Phrase Coreference Research: The First Fifteen Years, ACL 2010.](./CR/P10-1142.pdf)
2. [Kenton Lee at al.: End-to-end Neural Coreference Resolution, EMNLP 2017.](./CR/1707.07045v1.pdf)


# Machine Translation & Transliteration, Sequence-to-Sequence Models

1. [Peter F. Brown et al.: A Statistical Approach to Machine Translation, Computational Linguistics, 1990.](./MTS/Brown1990.pdf)
2. [Kevin Knight, Graehl Jonathan. Machine Transliteration. Computational Linguistics, 1992.](./MTS/J98-4003.pdf)
3. [Dekai Wu: Inversion Transduction Grammars and the Bilingual Parsing of Parallel Corpora, Computational Linguistics, 1997.](./MTS/J97-3002.pdf)
4. [Kevin Knight: A Statistical MT Tutorial Workbook, 1999.](./MTS/Knight1999.pdf)
5. [Kishore Papineni, et al.: BLEU: a Method for Automatic Evaluation of Machine Translation, ACL 2002.](./MTS/10.1.1.19.9416.pdf)
6. [Philipp Koehn, Franz J Och, and Daniel Marcu: Statistical Phrase-Based Translation, NAACL 2003.](./MTS/N03-1017.pdf)
7. [Philip Resnik and Noah A. Smith: The Web as a Parallel Corpus, Computational Linguistics, 2003.](./MTS/resnik_smith.pdf)
8. [Franz J Och and Hermann Ney: The Alignment-Template Approach to Statistical Machine Translation, Computational Linguistics, 2004.](./MTS/T07.0.pdf)
9. [David Chiang. A Hierarchical Phrase-Based Model for Statistical Machine Translation, ACL 2005.](./MTS/P05-1033.pdf)
10. [Ilya Sutskever, Oriol Vinyals, and Quoc V. Le: Sequence to Sequence Learning with Neural Networks, NIPS 2014.](./MTS/NIPS-2014-sequence-to-sequence-learning-with-neural-networks-Paper.pdf)
11. [Oriol Vinyals, Quoc Le: A Neural Conversation Model, 2015.](./MTS/1506.05869.pdf)
12. [Dzmitry Bahdanau, et al.: Neural Machine Translation by Jointly Learning to Align and Translate, 2014.](./MTS/1409.0473.pdf)
13. [Minh-Thang Luong, et al.: Effective Approaches to Attention-based Neural Machine Translation, 2015.](./MTS/emnlp15_attn.pdf)
14. [Rico Sennrich et al.: Neural Machine Translation of Rare Words with Subword Units. ACL 2016.](./MTS/1508.07909.pdf)
15. [Yonghui Wu, et al.: Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation, 2016.](./MTS/1609.08144v1.pdf)
16. [Melvin Johnson, et al.: Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation, 2016.](./MTS/1611.04558.pdf)
17. [Jonas Gehring, et al.: Convolutional Sequence to Sequence Learning, 2017.](./MTS/gehring17a.pdf)
18. [Ashish Vaswani, et al.: Attention Is All You Need, 2017.](./MTS/NIPS-2017-attention-is-all-you-need-Paper.pdf)


# Sequential Labeling & Information Extraction

1. [Marti A. Hearst: Automatic Acquisition of Hyponyms from Large Text Corpora, COLING 1992.](./SI/10.1.1.36.701.pdf)
2. [Collins and Singer: Unsupervised Models for Named Entity Classification, EMNLP 1999.](./SI/W99-0613.pdf)
3. [Patrick Pantel and Dekang Lin, Discovering Word Senses from Text, SIGKDD, 2002.](./SI/kdd02.pdf)
4. [Mike Mintz et al.: Distant supervision for relation extraction without labeled data, ACL 2009.](./SI/mintz.pdf)
5. [Zhiheng Huang et al.: Bidirectional LSTM-CRF Models for Sequence Tagging, 2015.](./SI/1508.01991.pdf)
6. [Xuezhe Ma and Eduard Hovy: End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF, ACL 2016.](./SI/P16-1101.pdf)


# Segmentation, Tagging, Parsing

1. [Donald Hindle and Mats Rooth. Structural Ambiguity and Lexical Relations, Computational Linguistics, 1993.](./STP/P91-1030.pdf)
2. [Adwait Ratnaparkhi: A Maximum Entropy Model for Part-Of-Speech Tagging, EMNLP 1996.](./STP/Ratnaparkhi_96.pdf)
3. [Eugene Charniak: A Maximum-Entropy-Inspired Parser, NAACL 2000.](./STP/974305.974323.pdf)
4. [Michael Collins: Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms, EMNLP 2002.](./STP/tagperc.pdf)
5. [Dan Klein and Christopher Manning: Accurate Unlexicalized Parsing, ACL 2003.](./STP/unlexicalized-parsing.pdf)
6. [Joakim Nivre and Mario Scholz: Deterministic Dependency Parsing of English Text, COLING 2004.](./STP/Deterministic_dependency_parsing_of_English_text)
7. [Ryan McDonald et al.: Non-Projective Dependency Parsing using Spanning-Tree Algorithms, EMNLP 2005.](./STP/nonprojectiveHLT-EMNLP2005.pdf)
8. [Daniel Andor et al.: Globally Normalized Transition-Based Neural Networks, 2016.](./STP/1603.06042.pdf)
9. [Oriol Vinyals, et al.: Grammar as a Foreign Language, 2015.](./STP/1412.7449.pdf)


# Language Modeling

1. [Joshua Goodman: A bit of progress in language modeling, MSR Technical Report, 2001.](./LM/0108005v1.pdf)
2. [Stanley F. Chen and Joshua Goodman: An Empirical Study of Smoothing Techniques for Language Modeling, ACL 2006.](./LM/10.1.1.335.8193.pdf)
3. [Yee Whye Teh: A Bayesian interpretation of Interpolated Kneser-Ney, 2006.](./LM/10.1.1.105.5397.pdf)
4. [Yoshua Bengio, et al.: A Neural Probabilistic Language Model, J. of Machine Learning Research, 2003.](./LM/944919.944966.pdf)
5. [Andrej Karpathy: The Unreasonable Effectiveness of Recurrent Neural Networks, 2015.](./LM/KarparthyUNREASONABLY-EFFECTIVE-RNN-15.pdf)
6. [Yoon Kim, et al.: Character-Aware Neural Language Models, 2015.](./LM/1508.06615.pdf)
7. [Alec Radford, et al.: Language Models are Unsupervised Multitask Learners, 2018.](./LM/language_models_are_unsupervised_multitask_learners.pdf)


# Topic Models

1. [Thomas Hofmann: Probabilistic Latent Semantic Indexing, SIGIR 1999.](./TM/download.pdf)
2. [David Blei, Andrew Y. Ng, and Michael I. Jordan: Latent Dirichlet Allocation, J. Machine Learning Research, 2003.](./TM/lda.pdf)


# Clustering & Word/Sentence Embeddings

1. [Peter F Brown, et al.: Class-Based n-gram Models of Natural Language, 1992.](./CWSE/lsvm-pami.pdf)
2. [Tomas Mikolov, et al.: Efficient Estimation of Word Representations in Vector Space, 2013.](./CWSE/1301.3781.pdf)
3. [Tomas Mikolov, et al.: Distributed Representations of Words and Phrases and their Compositionality, NIPS 2013.](./CWSE/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf)
4. [Quoc V. Le and Tomas Mikolov: Distributed Representations of Sentences and Documents, 2014.](./CWSE/le14.pdf)
5. [Jeffrey Pennington, et al.: GloVe: Global Vectors for Word Representation, 2014.](./CWSE/glove.pdf)
6. [Ryan Kiros, et al.: Skip-Thought Vectors, 2015.](./CWSE/1506.06726.pdf)
7. [Piotr Bojanowski, et al.: Enriching Word Vectors with Subword Information, 2017.](./CWSE/1607.04606v2.pdf)
8. [Daniel Cer et al.: Universal Sentence Encoder, 2018.](./CWSE/NIPS-2013-distributed-representations-of-words-and-phrases-and-their-compositionality-Paper.pdf)


# Neural Models

1. [Richard Socher, et al.: Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection, NIPS 2011.](./NM/SocherHuangPenningtonNgManning_NIPS2011.pdf)
2. [Ronan Collobert et al.: Natural Language Processing (almost) from Scratch, J. of Machine Learning Research, 2011.](./NM/1103.0398.pdf)
3. [Richard Socher, et al.: Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank, EMNLP 2013.](./NM/EMNLP2013_RNTN.pdf)
4. [Xiang Zhang, Junbo Zhao, and Yann LeCun: Character-level Convolutional Networks for Text Classification, NIPS 2015.](./NM/NIPS-2015-character-level-convolutional-networks-for-text-classification-Paper.pdf)
5. [Yoon Kim: Convolutional Neural Networks for Sentence Classification, 2014.](./NM/EMNLP2014181.pdf)
6. [Christopher Olah: Understanding LSTM Networks, 2015.](./NM/1503.04069.pdf)
7. [Matthew E. Peters, et al.: Deep contextualized word representations, 2018.](./NM/Deep_contextualized_word_representations.pdf)
8. [Jacob Devlin, et al.: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2018.](./NM/1810.04805.pdf)
9. [Yihan Liu et al. RoBERTa: A Robustly Optimized BERT Pretraining Approach, 2020.](./NM/1907.11692.pdf)


# Machine Learning

1. [Avrim Blum and Tom Mitchell: Combining Labeled and Unlabeled Data with Co-Training, 1998.](./ML/10.1.1.156.4813.pdf)
2. [John Lafferty, Andrew McCallum, Fernando C.N. Pereira: Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data, ICML 2001.](./ML/fulltext.pdf)
3. [Charles Sutton, Andrew McCallum. An Introduction to Conditional Random Fields for Relational Learning.](./ML/crf-tutorial.pdf)
4. [Kamal Nigam, et al.: Text Classification from Labeled and Unlabeled Documents using EM. Machine Learning, 1999.](./ML/emcat-mlj99.pdf)
5. [Kevin Knight: Bayesian Inference with Tears, 2009.](./ML/bayes-fst.pdf)
6. [Marco Tulio Ribeiro et al.: "Why Should I Trust You?": Explaining the Predictions of Any Classifier, KDD 2016.](./ML/1602.04938v3.pdf)
7. [Marco Tulio Ribeiro et al.: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList, ACL 2020.](./ML/2020.acl-main.442.pdf)


# Generation, Reinforcement Learning

1. [Jiwei Li, et al.: Deep Reinforcement Learning for Dialogue Generation, EMNLP 2016.](./G&RL/1606.01541.pdf)
2. [Marcâ€™Aurelio Ranzato et al.: Sequence Level Training with Recurrent Neural Networks. ICLR 2016.](./G&RL/1511.06732.pdf)
3. [Samuel R Bowman et al.: Generating sentences from a continuous space, CoNLL 2016.](./G&RL/1511.06349.pdf)
4. [Lantao Yu, et al.: SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient, AAAI 2017.](./G&RL/1609.05473.pdf)